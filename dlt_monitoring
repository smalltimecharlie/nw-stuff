# Databricks DLT Pipeline Monitoring API Utilities
# Script to track pipeline runs, durations, status, and provide monitoring insights.

import requests
import pandas as pd
from datetime import datetime
from collections import Counter
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, from_json, max as spark_max, unix_timestamp

# ---------- CONFIG ----------
DATABRICKS_HOST = "https://<your-workspace>.cloud.databricks.com"
TOKEN = dbutils.secrets.get(scope="monitoring", key="dlt_token")
PIPELINE_ID = "<your-pipeline-id>"
EVENT_LOG_PATH = "s3a://your-pipeline-storage/system/events/"  # Adjust if using DBFS
S3_PATH = "s3a://your-ingest-bucket/prefix/"  # Adjust as needed

HEADERS = {"Authorization": f"Bearer {TOKEN}"}
spark = SparkSession.getActiveSession()

# ---------- API HELPERS ----------
def get_pipeline_updates():
    url = f"{DATABRICKS_HOST}/api/2.0/pipelines/{PIPELINE_ID}/updates"
    response = requests.get(url, headers=HEADERS)
    response.raise_for_status()
    return response.json().get("updates", [])

def get_pipeline_config():
    url = f"{DATABRICKS_HOST}/api/2.0/pipelines/{PIPELINE_ID}"
    response = requests.get(url, headers=HEADERS)
    response.raise_for_status()
    return response.json()

def get_update_details(update_id):
    url = f"{DATABRICKS_HOST}/api/2.0/pipelines/{PIPELINE_ID}/updates/{update_id}"
    response = requests.get(url, headers=HEADERS)
    response.raise_for_status()
    return response.json()

def trigger_pipeline(full_refresh=False):
    url = f"{DATABRICKS_HOST}/api/2.0/pipelines/{PIPELINE_ID}/updates"
    response = requests.post(url, headers=HEADERS, json={"full_refresh": full_refresh})
    response.raise_for_status()
    return response.json()

# ---------- METRIC COLLECTION ----------
updates = get_pipeline_updates()

# Run states summary
state_counts = Counter([u["state"] for u in updates])
print("Run state summary:", state_counts)

# Run durations
print("
Run durations:")
for u in updates:
    update_id = u['update_id']
    started = datetime.fromtimestamp(u["creation_time"] / 1000)
    try:
        details = get_update_details(update_id)
        completed_ts = details.get("update_state", {}).get("timestamp")
        if completed_ts:
            ended = datetime.fromtimestamp(completed_ts / 1000)
        else:
            ended = datetime.now()
    except Exception as e:
        print(f"Warning: couldn't fetch details for update {update_id}: {e}")
        ended = datetime.now()
    duration = ended - started
    print(f"{update_id} took {duration} ({u['state']}) [started at {started}, ended at {ended}]")


latest = updates[0]
if latest["state"] != "COMPLETED":
    print(f"\nðŸš¨ Pipeline FAILED or not complete: {latest['state']} â€” Cause: {latest.get('cause', 'Unknown')}")

# Save updates to Delta
df = pd.DataFrame(updates)
spark_df = spark.createDataFrame(df)
spark_df.write.mode("overwrite").saveAsTable("monitoring.dlt_run_history")

# ---------- PIPELINE CONFIG SNAPSHOT ----------
config = get_pipeline_config()
print("\nPipeline configuration:")
print("Name:", config.get("name"))
print("Channel:", config.get("channel"))
print("Storage:", config.get("storage"))
print("Target schema:", config.get("target"))
print("Edition:", config.get("edition"))
print("Configured tables:", config.get("tables", []))
print("Clusters:", config.get("clusters", []))

# ---------- ESTIMATE FILES WAITING TO BE PROCESSED (via event log) ----------
print("\nEstimating unprocessed files using DLT event log...")

# Load the event log
event_df = spark.read.format("delta").load(EVENT_LOG_PATH)
flow_progress_df = event_df.filter(col("event_type") == "flow_progress")

# Flatten processed files from event details
processed_df = (
    flow_progress_df
    .filter(col("details.sources").isNotNull())
    .select(explode("details.sources").alias("source"))
    .filter(col("source.processedFiles").isNotNull())
    .select(explode("source.processedFiles").alias("file_path"))
    .distinct()
)
processed_files = [row.file_path for row in processed_df.collect()]
print(f"Processed files recorded in event log: {len(processed_files)}")

# List files from S3 source
s3_df = spark.read.format("binaryFile").load(S3_PATH)
s3_info_df = s3_df.select(col("path"), col("modificationTime"))
s3_paths = [row.path for row in s3_info_df.collect()]

# Determine unprocessed files
unprocessed = s3_info_df.filter(~col("path").isin(processed_files))
unprocessed_count = unprocessed.count()
print(f"Unprocessed files (in S3 but not in event log): {unprocessed_count}")
print("Sample:", unprocessed.select("path").limit(5).toPandas())

# ---------- INGESTION LAG METRICS ----------
print("\nCalculating ingestion lag from event log...")

lag_df = (
    flow_progress_df
    .filter(col("details.sources").isNotNull())
    .select(explode("details.sources").alias("source"), col("timestamp"))
    .filter(col("source.processedFiles").isNotNull())
    .select(col("timestamp"), explode(col("source.processedFiles")).alias("file"))
    .join(s3_df.select(col("path"), col("modificationTime")), s3_df.path == col("file"), "inner")
    .select(
        col("file"),
        col("timestamp").alias("ingestion_time"),
        col("modificationTime").alias("source_mod_time"),
        (unix_timestamp("timestamp") - unix_timestamp("modificationTime")).alias("ingestion_lag_sec")
    )
)

lag_df.createOrReplaceTempView("dlt_ingestion_lag")

lag_stats = spark.sql("""
    SELECT 
        COUNT(*) AS file_count,
        MAX(ingestion_lag_sec) AS max_lag_sec,
        AVG(ingestion_lag_sec) AS avg_lag_sec
    FROM dlt_ingestion_lag
""")

lag_stats.show()

print("\nDLT pipeline monitoring summary complete.")
